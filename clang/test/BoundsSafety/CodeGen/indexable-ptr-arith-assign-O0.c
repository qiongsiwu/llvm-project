// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --replace-value-regex "!annotation ![0-9]+" "!tbaa ![0-9]+" "!tbaa\.struct ![0-9]+" "!nosanitize ![0-9]+" "!srcloc ![0-9]+"
// RUN: %clang_cc1 -O0 -triple x86_64 -fbounds-safety -emit-llvm %s -o - | FileCheck %s
// RUN: %clang_cc1 -O0 -triple x86_64 -fbounds-safety -x objective-c -fexperimental-bounds-safety-objc -emit-llvm %s -o - | FileCheck %s

#include <ptrcheck.h>

// CHECK-LABEL: @test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca %"__bounds_safety::wide_ptr.indexable", align 8
// CHECK-NEXT:    [[P:%.*]] = alloca %"__bounds_safety::wide_ptr.indexable", align 8
// CHECK-NEXT:    [[INDEX_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca %"__bounds_safety::wide_ptr.indexable", align 8
// CHECK-NEXT:    [[TMP0:%.*]] = getelementptr inbounds { ptr, ptr }, ptr [[P]], i32 0, i32 0
// CHECK-NEXT:    store ptr [[P_COERCE0:%.*]], ptr [[TMP0]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds { ptr, ptr }, ptr [[P]], i32 0, i32 1
// CHECK-NEXT:    store ptr [[P_COERCE1:%.*]], ptr [[TMP1]], align 8
// CHECK-NEXT:    store i32 [[INDEX:%.*]], ptr [[INDEX_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[INDEX_ADDR]], align 4
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[TMP]], ptr align 8 [[P]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.indexable", ptr [[TMP]], i32 0, i32 0
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[TMP3]], align 8
// CHECK-NEXT:    [[BOUND_PTR_ARITH_OLD:%.*]] = ptrtoint ptr [[TMP4]] to i64
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP2]] to i64
// CHECK-NEXT:    [[BOUND_PTR_ARITH:%.*]] = getelementptr i32, ptr [[TMP4]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.indexable", ptr [[P]], i32 0, i32 0
// CHECK-NEXT:    store ptr [[BOUND_PTR_ARITH]], ptr [[TMP5]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.indexable", ptr [[TMP]], i32 0, i32 1
// CHECK-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[TMP6]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.indexable", ptr [[P]], i32 0, i32 1
// CHECK-NEXT:    store ptr [[TMP7]], ptr [[TMP8]], align 8
// CHECK-NEXT:    [[BOUND_PTR_ARITH_NEW:%.*]] = ptrtoint ptr [[BOUND_PTR_ARITH]] to i64
// CHECK-NEXT:    [[TMP9:%.*]] = icmp uge i64 [[BOUND_PTR_ARITH_NEW]], [[BOUND_PTR_ARITH_OLD]], {{!annotation ![0-9]+}}
// CHECK-NEXT:    br i1 [[TMP9]], label [[CONT:%.*]], label [[TRAP:%.*]], {{!annotation ![0-9]+}}
// CHECK:       trap:
// CHECK-NEXT:    call void @llvm.ubsantrap(i8 25) #[[ATTR3:[0-9]+]], {{!annotation ![0-9]+}}
// CHECK-NEXT:    unreachable
// CHECK:       cont:
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[RETVAL]], ptr align 8 [[P]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP10:%.*]] = load { ptr, ptr }, ptr [[RETVAL]], align 8
// CHECK-NEXT:    ret { ptr, ptr } [[TMP10]]
//
int *__indexable test(int *__indexable p, int index) {
  p += index;
  return p;
}

