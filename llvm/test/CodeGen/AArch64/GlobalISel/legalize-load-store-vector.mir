# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
# RUN: llc -O0 -mtriple=aarch64 -verify-machineinstrs -run-pass=legalizer -global-isel-abort=0 -pass-remarks-missed='gisel.*' -o - %s 2> %t.err | FileCheck %s
# RUN: FileCheck -check-prefix=ERR %s < %t.err

# ERR: remark: <unknown>:0:0: unable to legalize instruction: G_STORE %{{[0-9]+}}:_(<8 x s9>), %{{[0-9]+}}:_(p0) :: (store (<8 x s9>), align 16) (in function: store-narrow-non-byte-sized-s9)
# ERR-NEXT: remark: <unknown>:0:0: unable to legalize instruction: G_STORE %{{[0-9]+}}:_(<40 x s1>), %{{[0-9]+}}:_(p0) :: (store (<40 x s1>), align 8) (in function: store-narrow-non-byte-sized-s1)
# ERR-NEXT: remark: <unknown>:0:0: unable to legalize instruction: %{{[0-9]+}}:_(<8 x s9>) = G_LOAD %{{[0-9]+}}:_(p0) :: (load (<8 x s9>), align 16) (in function: load-narrow-non-byte-sized-s9)
# ERR-NEXT: remark: <unknown>:0:0: unable to legalize instruction: %{{[0-9]+}}:_(<40 x s1>) = G_LOAD %{{[0-9]+}}:_(p0) :: (load (<40 x s1>), align 8) (in function: load-narrow-non-byte-sized-s1)

# FIXME: Non-byte-sized vector elements cause fallback in LegalizerHelper::reduceLoadStoreWidth
---
name:            store-narrow-non-byte-sized-s9
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $x8
    ; CHECK-LABEL: name: store-narrow-non-byte-sized-s9
    ; CHECK: liveins: $x8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(p0) = COPY $x8
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s9) = G_CONSTANT i9 -256
    ; CHECK-NEXT: [[C1:%[0-9]+]]:_(s9) = G_CONSTANT i9 -255
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<8 x s9>) = G_BUILD_VECTOR [[C]](s9), [[C1]](s9), [[C]](s9), [[C1]](s9), [[C]](s9), [[C1]](s9), [[C]](s9), [[C1]](s9)
    ; CHECK-NEXT: G_STORE [[BUILD_VECTOR]](<8 x s9>), [[COPY]](p0) :: (store (<8 x s9>), align 16)
    ; CHECK-NEXT: RET_ReallyLR
    %0:_(p0) = COPY $x8
    %1:_(s9) = G_CONSTANT i9 256
    %2:_(s9) = G_CONSTANT i9 257
    %3:_(<8 x s9>) = G_BUILD_VECTOR %1(s9), %2(s9), %1(s9), %2(s9), %1(s9), %2(s9), %1(s9), %2(s9)
    G_STORE %3(<8 x s9>), %0(p0) :: (store (<8 x s9>), align 16)
    RET_ReallyLR
...
---
name:            store-narrow-non-byte-sized-s1
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $x8
    ; CHECK-LABEL: name: store-narrow-non-byte-sized-s1
    ; CHECK: liveins: $x8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(p0) = COPY $x8
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s1) = G_CONSTANT i1 false
    ; CHECK-NEXT: [[C1:%[0-9]+]]:_(s1) = G_CONSTANT i1 true
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<40 x s1>) = G_BUILD_VECTOR [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1), [[C]](s1), [[C1]](s1)
    ; CHECK-NEXT: G_STORE [[BUILD_VECTOR]](<40 x s1>), [[COPY]](p0) :: (store (<40 x s1>), align 8)
    ; CHECK-NEXT: RET_ReallyLR
    %0:_(p0) = COPY $x8
    %1:_(s1) = G_CONSTANT i1 0
    %2:_(s1) = G_CONSTANT i1 1
    %3:_(<40 x s1>) = G_BUILD_VECTOR %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1), %1(s1), %2(s1)
    G_STORE %3(<40 x s1>), %0(p0) :: (store (<40 x s1>), align 8)
    RET_ReallyLR
...

# FIXME: Non-byte-sized vector elements cause fallback in LegalizerHelper::reduceLoadStoreWidth
---
name:            load-narrow-non-byte-sized-s9
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $x8
    ; CHECK-LABEL: name: load-narrow-non-byte-sized-s9
    ; CHECK: liveins: $x8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(p0) = COPY $x8
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(<8 x s9>) = G_LOAD [[COPY]](p0) :: (load (<8 x s9>), align 16)
    ; CHECK-NEXT: [[ZEXT:%[0-9]+]]:_(<8 x s16>) = G_ZEXT [[LOAD]](<8 x s9>)
    ; CHECK-NEXT: $q0 = COPY [[ZEXT]](<8 x s16>)
    ; CHECK-NEXT: RET_ReallyLR implicit $q0
    %0:_(p0) = COPY $x8
    %2:_(<8 x s9>) = G_LOAD %0(p0) :: (load (<8 x s9>), align 16)
    %3:_(<8 x s16>) = G_ZEXT %2(<8 x s9>)
    $q0 = COPY %3(<8 x s16>)
    RET_ReallyLR implicit $q0
...
---
name:            load-narrow-non-byte-sized-s1
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $x8
    ; CHECK-LABEL: name: load-narrow-non-byte-sized-s1
    ; CHECK: liveins: $x8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(p0) = COPY $x8
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(<40 x s1>) = G_LOAD [[COPY]](p0) :: (load (<40 x s1>), align 8)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:_(s1), [[UV1:%[0-9]+]]:_(s1), [[UV2:%[0-9]+]]:_(s1), [[UV3:%[0-9]+]]:_(s1), [[UV4:%[0-9]+]]:_(s1), [[UV5:%[0-9]+]]:_(s1), [[UV6:%[0-9]+]]:_(s1), [[UV7:%[0-9]+]]:_(s1), [[UV8:%[0-9]+]]:_(s1), [[UV9:%[0-9]+]]:_(s1), [[UV10:%[0-9]+]]:_(s1), [[UV11:%[0-9]+]]:_(s1), [[UV12:%[0-9]+]]:_(s1), [[UV13:%[0-9]+]]:_(s1), [[UV14:%[0-9]+]]:_(s1), [[UV15:%[0-9]+]]:_(s1), [[UV16:%[0-9]+]]:_(s1), [[UV17:%[0-9]+]]:_(s1), [[UV18:%[0-9]+]]:_(s1), [[UV19:%[0-9]+]]:_(s1), [[UV20:%[0-9]+]]:_(s1), [[UV21:%[0-9]+]]:_(s1), [[UV22:%[0-9]+]]:_(s1), [[UV23:%[0-9]+]]:_(s1), [[UV24:%[0-9]+]]:_(s1), [[UV25:%[0-9]+]]:_(s1), [[UV26:%[0-9]+]]:_(s1), [[UV27:%[0-9]+]]:_(s1), [[UV28:%[0-9]+]]:_(s1), [[UV29:%[0-9]+]]:_(s1), [[UV30:%[0-9]+]]:_(s1), [[UV31:%[0-9]+]]:_(s1), [[UV32:%[0-9]+]]:_(s1), [[UV33:%[0-9]+]]:_(s1), [[UV34:%[0-9]+]]:_(s1), [[UV35:%[0-9]+]]:_(s1), [[UV36:%[0-9]+]]:_(s1), [[UV37:%[0-9]+]]:_(s1), [[UV38:%[0-9]+]]:_(s1), [[UV39:%[0-9]+]]:_(s1) = G_UNMERGE_VALUES [[LOAD]](<40 x s1>)
    ; CHECK-NEXT: [[MV:%[0-9]+]]:_(s40) = G_MERGE_VALUES [[UV]](s1), [[UV1]](s1), [[UV2]](s1), [[UV3]](s1), [[UV4]](s1), [[UV5]](s1), [[UV6]](s1), [[UV7]](s1), [[UV8]](s1), [[UV9]](s1), [[UV10]](s1), [[UV11]](s1), [[UV12]](s1), [[UV13]](s1), [[UV14]](s1), [[UV15]](s1), [[UV16]](s1), [[UV17]](s1), [[UV18]](s1), [[UV19]](s1), [[UV20]](s1), [[UV21]](s1), [[UV22]](s1), [[UV23]](s1), [[UV24]](s1), [[UV25]](s1), [[UV26]](s1), [[UV27]](s1), [[UV28]](s1), [[UV29]](s1), [[UV30]](s1), [[UV31]](s1), [[UV32]](s1), [[UV33]](s1), [[UV34]](s1), [[UV35]](s1), [[UV36]](s1), [[UV37]](s1), [[UV38]](s1), [[UV39]](s1)
    ; CHECK-NEXT: [[ZEXT:%[0-9]+]]:_(s64) = G_ZEXT [[MV]](s40)
    ; CHECK-NEXT: $x0 = COPY [[ZEXT]](s64)
    ; CHECK-NEXT: RET_ReallyLR implicit $x0
    %0:_(p0) = COPY $x8
    %1:_(<40 x s1>) = G_LOAD %0:_(p0) :: (load (<40 x s1>), align 8)
    %2:_(s40) = G_BITCAST %1:_(<40 x s1>)
    %3:_(s64) = G_ZEXT %2(s40)
    $x0 = COPY %3(s64)
    RET_ReallyLR implicit $x0
...
